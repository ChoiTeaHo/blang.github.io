---
title:  "라즈베리파이(16) WEB WAS 세션클러스터링"
excerpt: "라즈베리파이(16) WEB WAS 세션클러스터링 입니다."
categories:
  - web-service-dev
tags:
  - [web, dev]
toc: true
toc_sticky: true

last_modified_at: 2022-10-12T20:00:00-05:00
---

# 라즈베리파이 WEB WAS 세션클러스터링
## 세션클러스터링 개요
  : 라즈베리파이에 로드밸런싱이 적용된 Apache & Tomcat서버에 세션클러스터링을 적용한다.

  - 적용전 로드밸런싱 상태
    : 번갈아가며(로드밸런싱) WAS1,2 의 문서를 출력하며, 최소한의 세션유지위해 한번 요청되어 처리한 서버가 존재하면 그쪽 서버로만 요청하도록 sticky 밸런싱을 설정한 상태이다.
  
  - 문제점
    : 만약 sticky 하게 한쪽 WAS만 바라보던 상황에서에서 `fail` 이 발생하면 다른 WAS를 바라보게 되는데 이때 유지하던 세션은 잃어버리는 문제점이 있다. 또한 `fail` 이 발생하지 않는다 하더라도 고정적(sticky) 으로 설정해두었기에 한쪽 서버만 바라보면서 오히려 부하가 발생할 수 있다.

  - 세션클러스터링 적용 후
    : 세션이 사용중이던 한쪽 WAS가 끊어지고 로드밸런싱으로 다른 측의 WAS를 바라볼때 끊어진 WAS측의 세션을 공유하도록 한다.



## 세션클러스터링 문제점

  L4 스위치의 경우 사용자는 접속했던 WAS로 접속을 유도해주지만 하나의 WAS에서 허용된 동접수를 초과한 접속이 발생할 경우 다른쪽으로의 접속을 유도해주게 됩니다.
  {: .notice--info}


## 세션클러스터링 구축
### 아파치재단에서 말하는 세션클러스터링 구축
  : 아파치사의 mod_jk 는 세션클러스터링에 대해 다음과 같은 특징을 가진다고 한다.

  - multicast 방식으로 동작하며 address는 ‘228.0.0.4`, port는 ‘45564’를 사용하고 서버 IP는 java.net.InetAddress.getLocalHost().getHostAddress()로 얻어진 IP 값으로 송출됩니다.
  - 먼저 구동되는 서버부터 4000 ~ 4100 사이의 TCP port를 통해 reqplication message를 listening합니다.
  - Listener는 ClusterSessionListener, interceptor는 TcpFailureDetector와 MessageDispatchInterceptor가 설정됩니다.


### WAS서버 세션클러스터링 설정
  : 와스서버(톰캣) 측에서 변경 작업을 한다.

  1. (톰캣루트/conf/server.xml) 설정  
    : 클러스터링 관련 내용 주석을 해제하고 설정을 작성한다.

      ![사진1](/assets/images/ToyDev/WebServiceDev/tomcat_session_cluster_1.jpg)

      `<Cluster>` 태그 사이에는 `<Membership>`, `<Receiver>`, `<Sender>` 라는 3개의 태그가 위치한다.   
      - `<Membership>`: 멤버 그룹을 정의 하는 것으로 해당 값이 동일한 모든 톰캣 프로세스는 클러스터로묶이게 된다. 
      - `<Receiver>`: 클러스터 그룹에서 보내오는 메시지와 세션 정보 등을 받아오는 것이며
      - `<Sender>`: 자신의 세션 정보 및 메시지를 전송하는 것이다.



      ```bash
      # (WAS 1번에서 작성)

      <Cluster className="org.apache.catalina.ha.tcp.SimpleTcpCluster" channelSendOptions="8">
      <Manager className="org.apache.catalina.ha.session.DeltaManager" expireSessionsOnShutdown="false" notifyListenersOnReplication="true"/>
      <Channel className="org.apache.catalina.tribes.group.GroupChannel">

        <Membership className="org.apache.catalina.tribes.membership.McastService"
                    address="228.0.0.4"
                    port="45564"
                    frequency="500"
                    dropTime="3000"/>
        <Receiver className="org.apache.catalina.tribes.transport.nio.NioReceiver" 
                    address="192.168.0.10" 
                    port="4000" 
                    autoBind="100" 
                    selectorTimeout="5000"
                    maxThreads="6"/>
        <Sender className="org.apache.catalina.tribes.transport.ReplicationTransmitter">
          <Transport className="org.apache.catalina.tribes.transport.nio.PooledParallelSender"/>
        </Sender>
        <Interceptor className="org.apache.catalina.tribes.group.interceptors.TcpFailureDetector"/>
        <Interceptor className="org.apache.catalina.tribes.group.interceptors.MessageDispatchInterceptor"/>
      </Channel>

      <Valve className="org.apache.catalina.ha.tcp.ReplicationValve" filter=""/>
      <Valve className="org.apache.catalina.ha.session.JvmRouteBinderValve"/>
     
      <Deployer className="org.apache.catalina.ha.deploy.FarmWarDeployer" 
                tempDir="/tmp/war-temp/"
                deployDir="/tmp/war-deploy/"
                watchDir="/tmp/war-listen/"
                watchEnabled="false"/>

      <ClusterListener className="org.apache.catalina.ha.session.ClusterSessionListener"/>
      </Cluster>

      ```
  
  
      ```bash
      # (WAS 2번에서 작성)

      <Cluster className="org.apache.catalina.ha.tcp.SimpleTcpCluster" channelSendOptions="8">
      <Manager className="org.apache.catalina.ha.session.DeltaManager" expireSessionsOnShutdown="false" notifyListenersOnReplication="true"/>
      <Channel className="org.apache.catalina.tribes.group.GroupChannel">

        <Membership className="org.apache.catalina.tribes.membership.McastService"
                    address="228.0.0.4"
                    port="45564"
                    frequency="500"
                    dropTime="3000"/>
        <Receiver className="org.apache.catalina.tribes.transport.nio.NioReceiver" 
                    address="192.168.0.12" 
                    port="4001" 
                    autoBind="100" 
                    selectorTimeout="5000"
                    maxThreads="6"/>
        <Sender className="org.apache.catalina.tribes.transport.ReplicationTransmitter">
          <Transport className="org.apache.catalina.tribes.transport.nio.PooledParallelSender"/>
        </Sender>
        <Interceptor className="org.apache.catalina.tribes.group.interceptors.TcpFailureDetector"/>
        <Interceptor className="org.apache.catalina.tribes.group.interceptors.MessageDispatchInterceptor"/>
      </Channel>

      <Valve className="org.apache.catalina.ha.tcp.ReplicationValve" filter=""/>
      <Valve className="org.apache.catalina.ha.session.JvmRouteBinderValve"/>
     
      <Deployer className="org.apache.catalina.ha.deploy.FarmWarDeployer" 
                tempDir="/tmp/war-temp/"
                deployDir="/tmp/war-deploy/"
                watchDir="/tmp/war-listen/"
                watchEnabled="false"/>

      <ClusterListener className="org.apache.catalina.ha.session.ClusterSessionListener"/>
      </Cluster>

      ```

      꼭 WAS1, WSA2 모두 적용시켜주도록 한다.  
      별 다른이야기는 없고 `<Receiver>` 내부의 포트만 다르게 설정했음을 알 수 있다.
      {: .notice--info}

      
  2. (아파치루트/conf/httpd.conf) 설정  
    : 톰캣에 통과시킬 매핑을 정의하는 jk마운트를 최종 설정한다.

      ```bash
      # 최종으로 mod_jk JK마운트로 worker를 등록한다. url를 구분하여 톰캣이 처리할지 결정한다.
      # /* 으로들어오는 요청을 blang_balancer 라는 워커로 넘긴다.
      # 워커 설정에는 로드밸런싱이 설정되어있다.
      # tomcat1, tomcat2 골고루 요청을 분산해줄 것이다.

      # ASIS
      #jkMount /* lilo_lb

      # TOBE
      jkMount /* blang_balancer

      ```

     
### WAS서버 mod_jk 설정
  : 와스서버(톰캣) 측의 설정.

  1. (톰캣루트/conf/server.xml) 설정  
    : WAS 2대의 톰캣의 mod_jk 커넥터 포트를 겹치지 않도록 설정한다. 

***WAS1***

      ```bash
      #(WAS 1번)
      cd /fswas/tomcat/apache-tomcat-8.5.82/conf
      vi server.xml
      
      
      # 아래 mod_jk 로드밸런싱 전용 포트를 서버별로 각각 다르게 설정한다.
      <!-- Define an AJP 1.3 Connector on port 8009 -->
      <Connector protocol="AJP/1.3"
               address="0.0.0.0"
               secretRequired="false"
               # port="8009"     # 이전에 설정한 mod_jk 연동 포트이다. 주석처리하고
               port="18009"      # (#WAS1) 이부분은 로드밸런싱 내용이다. 이렇게 추가한다. (달라야한다)
               redirectPort="8443" />

      (중략)
      
      <!-- ASIS -->
      <!--
      <Engine name="Catalina" defaultHost="localhost">
      -->
  
      <!-- TOBE (로드밸런싱 사용시 jvmRoute 옵션으로 워커명을 준다.)-->
      <Engine name="Catalina" defaultHost="localhost" jvmRoute="tomcat1">
      
      ```

***WAS2***

      ```bash
      #(WAS 2번)
      cd /fswas/tomcat/apache-tomcat-8.5.82/conf
      vi server.xml
      
      
      # 아래 mod_jk 로드밸런싱 전용 포트를 서버별로 각각 다르게 설정한다.
      <!-- Define an AJP 1.3 Connector on port 8009 -->
      <Connector protocol="AJP/1.3"
               address="0.0.0.0"
               secretRequired="false"
               # port="8009"     # 이전에 설정한 mod_jk 연동 포트이다. 주석처리하고
               port="28009"      # (#WAS2) 이부분은 로드밸런싱 내용이다. 이렇게 추가한다. (달라야한다)
               redirectPort="8443" />

      (중략)
      
      <!-- ASIS -->
      <!--
      <Engine name="Catalina" defaultHost="localhost">
      -->
  
      <!-- TOBE (로드밸런싱 사용시 jvmRoute 옵션으로 워커명을 준다.)-->
      <Engine name="Catalina" defaultHost="localhost" jvmRoute="tomcat2">
      
      ```
      
### 연동완료 정리
  : 부하분산을 위한 mod_jk 의 로드밸런싱이 완료되었다. was1번,2번에 동일한 url서블릿 매핑을 가진 jsp주소를 호출해보면 (내용은 달라야한다) 서로 다른내용이 번갈아가며 출력되는것을 볼 수 있다.
  
![사진2](/assets/images/ToyDev/WebServiceDev/mod_jk_success_loadbalancer.png)

> ***참고) 세션 클러스터링이 필요하다.***  
> 아파치1, 톰캣2 구성의 로드밸런싱을 완성했다.
> 접속하면 WAS서버가 번갈아가면서 처리되는걸 보면 목적대로 로드밸런싱은 성공했다.
> 하지만 로그인 같은 경우 세션이 유지되어야하는데 이렇게 번갈아가면서 처리되면 안된다.
> 세션이 공유되어야하는 경우 세션 클러스터링 작업이 별도로 필요하다.
> 세션 클러스터링은 한쪽이 끊어져도 두개의 톰캣이 하나의 세션을 유지할 수 있도록 서로 공유한다.


### 정리

> 톰캣을 이용한 로드밸런싱과 세션 클러스터링에 대해서 알아보았다. 
> 일반적으로 로드밸런싱과 클러스터링은 성능 향상이라는 측면과 안정성 확보에 그 목적을 가지고 있다.
> 물론 고가의 상용 웹 어플리케이션 서버에 비하면 많이 부족하고 
> 하드웨어를 이용한 로드밸런싱과 클러스터링에 비하면 안정성이 떨어질 수도 있지만 
> 저렴한 비용으로 최대의 안정성과 성능을 얻고자 한다면 한번쯤 시도해 볼만한 좋은 기능이라고 할 수 있다.



